{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Neural Network Using PyTorch\n",
    "\n",
    "It consists of input features which are feed to the neuron, using combination of weights and bias. These inputs and weights are multiplied together to form weighted inputs and these weighted inputs are summed together to pass through an activation function.\n",
    "\n",
    "Activation Function - the function which decides whether the neuron should be activated or not. These can be used in hidden layers' neurons or output layer's neurons.<br>\n",
    "Here, Sigmoid Function is used as the Activation Function, narrowing the values between 0 and 1.\n",
    "\n",
    "### The Mathematical equations can be written as following : \n",
    "\n",
    "The output \"**y**\", is the sum of weighted inputs (where **x** represents inputs and **w**, weights) and bias (represented by **b**), passed through activation function \"**f**\". So, the equation is given as:\n",
    "$$y = f(x_{1}w_{1} + x_{2}w_{2} + .... + x_{n}w_{n} + b)$$<br>\n",
    "Since, here **sigmoid function** is used, we can rewrite the equation as:<br>\n",
    "$$y = \\sigma (x_{1}w_{1} + x_{2}w_{2} + .... + x_{n}w_{n} + b)$$\n",
    "<center>or</center>\n",
    "$$y = \\sigma (\\sum_{1}^{n} x_{i}w_{i} + b)$$\n",
    "\n",
    "And **Sigmoid Function** can be represented as:\n",
    "$$\\sigma = \\frac{1}{(1+e^{-x})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    \"\"\"\n",
    "        Sigmoid Activation Function\n",
    "        \n",
    "        Args:\n",
    "            x: torch.Tensor\n",
    "    \"\"\"\n",
    "    return 1/(1+torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of features:  torch.FloatTensor\n",
      "Features size:  torch.Size([1, 6])\n",
      "Features:  tensor([[-0.1468,  0.7861,  0.9468, -1.1143,  1.6908, -0.8948]])\n",
      "Weights:  tensor([[-0.3556,  1.2324,  0.1382, -1.6822,  0.3177,  0.1328]])\n",
      "Bias:  tensor([[0.1373]])\n"
     ]
    }
   ],
   "source": [
    "# Setting the seed to generate the random numbers\n",
    "torch.manual_seed(7)\n",
    "\n",
    "# Features are 6 random variables\n",
    "features = torch.randn((1, 6)) # this generates a tensor (1, 6), with 1 row and 6 column \n",
    "print(\"Type of features: \", features.type())\n",
    "print(\"Features size: \", features.shape)\n",
    "print(\"Features: \", features)\n",
    "# Generatning random weights\n",
    "weights = torch.randn((1, 6))\n",
    "print(\"Weights: \", weights)\n",
    "# Generating bias\n",
    "bias = torch.randn((1, 1))\n",
    "print(\"Bias: \", bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted inputs:  tensor([[3.4447]])\n",
      "Size of weightes inputs:  torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "# Multiplying the weights and features(inputs)\n",
    "weighted_inputs = torch.matmul(features, weights.view(6, 1))\n",
    "print(\"Weighted inputs: \", weighted_inputs)\n",
    "print(\"Size of weightes inputs: \", weighted_inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:  tensor([[0.9729]])\n"
     ]
    }
   ],
   "source": [
    "# Calculating the output \n",
    "\n",
    "# *************  METHOD 1 *****************\n",
    "output = activation(weighted_inputs + bias)\n",
    "print(\"Output: \", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9729]])\n"
     ]
    }
   ],
   "source": [
    "# ************** METHOD 2 ******************\n",
    "output = activation((features * weights).sum() + bias)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9729]])\n"
     ]
    }
   ],
   "source": [
    "# ************** METHOD 3 *******************\n",
    "output = activation(torch.mm(features, weights.view(6, 1)) + bias)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mulitlayer network \n",
    "Here, considering the network with 1 hidden layer, weights will be defined for input layer to hidden layer (W1) and from hidden layer to output layer (W2) and same goes for biases.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  tensor([[-1.8941,  1.0056, -0.6948,  0.9062,  0.1072]])\n",
      "Features shape:  torch.Size([1, 5]) \n",
      "\n",
      "W1:  tensor([[ 0.6125,  0.3296, -0.8763],\n",
      "        [-1.6768, -0.7247,  0.9634],\n",
      "        [ 0.1342,  0.5485,  2.1349],\n",
      "        [-0.8782, -2.0826,  1.8317],\n",
      "        [-0.5535,  1.0395, -1.2601]])\n",
      "W1 shape:  torch.Size([5, 3]) \n",
      "\n",
      "Bias1:  tensor([[ 0.4156, -1.3031,  0.4350]]) \n",
      "\n",
      "W2:  tensor([[-1.1498],\n",
      "        [-0.8150],\n",
      "        [-0.9118]])\n",
      "W2 shape:  torch.Size([3, 1])\n",
      "Bias2:  tensor([[-0.0739]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Features for the neural network \n",
    "features = torch.randn((1, 5))\n",
    "print(\"Features: \", features)\n",
    "print(\"Features shape: \", features.shape, \"\\n\")\n",
    "\n",
    "# Defining size of layers for the network\n",
    "n_input = features.shape[1]\n",
    "n_hidden = 3\n",
    "n_output = 1\n",
    "\n",
    "# Weights & bias: from input layer to hidden layer\n",
    "W1 = torch.randn(n_input, n_hidden)\n",
    "print(\"W1: \", W1)\n",
    "print(\"W1 shape: \", W1.shape, \"\\n\")\n",
    "b1 = torch.randn((1, n_hidden))\n",
    "print(\"Bias1: \", b1, \"\\n\")\n",
    "\n",
    "# Weights & bias: from hidden layer to output layer\n",
    "W2 = torch.randn(n_hidden, n_output)\n",
    "print(\"W2: \", W2)\n",
    "print(\"W2 shape: \", W2.shape)\n",
    "b2 = torch.randn((1, n_output))\n",
    "print(\"Bias2: \", b2, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted inputs for hidden layer:  tensor([[-3.7949, -3.5102,  2.6700]])\n",
      "Sum1:  tensor([[-3.3794, -4.8133,  3.1050]])\n"
     ]
    }
   ],
   "source": [
    "# Weighted inputs for hidden layer\n",
    "# Since features has size of (1x5) & weights (5x3), their resulting multiplication will have (1x3) \n",
    "weighted_inputs1 = torch.mm(features, W1)\n",
    "print(\"Weighted inputs for hidden layer: \", weighted_inputs1)\n",
    "# sum of weighted inputs and biases for hidden layer\n",
    "# size of weighted inputs is (1x3) & biases (1x3), sum1 will have (1x3) size.\n",
    "sum1 = weighted_inputs1 + b1\n",
    "print(\"Sum1: \", sum1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from hidden layer:  tensor([[0.0329, 0.0081, 0.9571]])\n",
      "Multiplication of hidden layer output and bias2:  tensor([[-0.9171]])\n",
      "Output from the output layer:  tensor([[0.2707]])\n"
     ]
    }
   ],
   "source": [
    "# Output from hidden layer\n",
    "output_hidden = activation(sum1)\n",
    "print(\"Output from hidden layer: \", output_hidden)\n",
    "# Final output from the neural network with a single hidden layer\n",
    "# Here for output layer, output_hidden will act as features and weights are given as W2\n",
    "# output_hidden: (1x3) & W2: (3x1), so matmul is (1x1) and bias2: (1x1), so final output will be (1x1)\n",
    "print(\"Multiplication of hidden layer output and bias2: \", torch.mm(output_hidden, W2))\n",
    "final_output = activation(torch.mm(output_hidden, W2) + b2)\n",
    "print(\"Output from the output layer: \", final_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
